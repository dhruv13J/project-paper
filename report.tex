%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{makeidx}
\usepackage{color}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{National Institute of Technology, Goa} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Abstractive Text Summarization \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Dhruv Jawali, Vighnesh Birodkar \\
\\
\small{under the guidance of} \\
\\
\\
Mrs. Veena T
} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}


\maketitle % Print the title
\pagebreak
\tableofcontents
\pagebreak




\section{Introduction}
\emph{Information Explosion} \cite{fshock} refers to the rapid increase in the amount of published information or data, and the effects of this abundance. The internet now consists of at least 4.3 billion web pages \cite{websize}. With each passing day, this number increases. Storing, interpreting and maintaining the ever increasing amount of information on the internet is going to be a challenge in the near future.\\
\par
Computers are ideal for storing and manipulating data, but lack the means to interpret it. Formally defining grammars for human languages is not possible, since the meanings of words are sensitive to the context. As a result, in the present scenario, the task of understanding and interpreting text data is a task primarily limitied to human subjects. Although data on the internet keeps growing exponentially, the same is neither true for the people overseeing this growth, nor for the people looking to access this information; for example, the flood of technical reviews when a new product is launched. Potential customers might have to sift through the dozens of review websites, looking for salient points in each review. It would be much easier if the information on these websites could be collected and compiled into a single, coherent document.\\
\par
Until computers are capable of interpreting human languages, humans must be kept in the loop. We need a system to limit the data the humans have to oversee, without omitting important facts. This is acheieved through \emph{Text Summarization Techniques}.\\
\par

With the current state-of-the-art in Natural Language Processing, it highly difficult, if not impossible, to design a system which derives semantic information from a text document. Hence, current text summarization techniques rely on statistical measures of importance, combined with heuristics, to assign scores to sentences within the document to create summaries. Some techniques based on forming new sentences, rather than just extracting sentences from the given set of documents, take advantage of parts-of-speech tagging, and identify semantic structure within documents. These structures, however, are limited to the type of documents being summarized, and are subject to the limitations faced by the NLP techniques used to describe them.\\
\par

An interesting way to look at the Text Summarization Problem is to pose it as a Machine Learning Problem. Using the statistical measures defined to order sentences in other extractive techniques, we model the importance of a sentence (a measure of whether it should appear in the summary or not) as a combination of these measures, and learn the weights assigned to them from human generated summaries. A regression technique is used for training, and a set of documents with summaries written by humans are used to generate scores for each sentence in the document.  In this project, we develop a novel way to generate scores for sentences in a document based on abstractive summaries written by humans, which are used to train a regression model. This model can then be used to generate extractive summaries. Evaluation of these summaries is done by generating ROUGE scores.
%TODO reference ROUGE, opinosis, radev. here
%---------------------------------------------------------------------------------------
\section{Text Summarization}
%TODO ref
Text summarization is the process of scanning a document, or a set of documents, and producing a summary which is shorter than the original document(s) which captures the important ideas expressed as much as possible. Most text summarization approaches define and try to optimize statistical criteria to select sentences from the document to be used to compose the final summary. The use of heuristics for the same is common as well.%TODO ref, read up on text summarization
Text Summarization has broadly been classified into the following two types. %TODO ref
\subsection{Extractive Text Summarization}
Given a document $D$ of $n$ sentences and a compression ratio $ 0 < r < 1 $ an \emph{Extractive Summarization Technique} selects $\lceil n \times r \rceil $  sentences which it think best represnt the document. It is called \emph{Extractive} summarization because it involves merely selecting sentences from the original document.
%TODO describe text rank and one other approach here.
\subsection{Abstractive Text Summarization}
Given a document $D$ of $n$ sentences and a compression ratio $ 0 < r < 1 $ an \emph{Abstractive Summarization Technique} produces $\lceil n \times r \rceil $  sentences which it think best represnt the document. The senteces may or may not be from the document. This approach is more likely to produce an accurate summary because it can represent facts with new and compact sentences rather than relying upon the sentences in the document. However, this requires that the summarazation technique to be grammar aware. Existing techniques implementing this approach are far from the abstractive capabilities of humans.

%TODO describe corpus and opinosis here

\section{Summarization as a Classification problem}
\emph{Extractive Summarization} can be thought of as a binary classification problem. Either the sentence in a document gets a label $1$ and is included in the summary, or it gets a label $0$ and is excluded from the summary. If we are able to represent each sentence by a numerical vector, numerous machine learning approaches can be used to assign the labels.
%TODO ref some ml approaches, maybe clustering
\\
\par
As mentioned earlier, an ideal summary can vary depending on the context: the person reading it or the type of document being summarized. A supervised machine learning approach is ideal in such a case. Given a document with labels ( of $0$ and $1$ ) a Machine Learning Technique can be used to learn from it and then predict the labels for each of the sentences of an unseen document. 

\subsection{Drawbacks of the Binary Classification approach}
According to the above mentioned approach a binary classifier can only be trained using a purely extractive summary. But extractive summaries are not readily available. A machine learning model with 

\bibliography{mybib}{}
\bibliographystyle{plain}


\end{document}
